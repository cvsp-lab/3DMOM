<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>3D-MOM</title>
    <style type="text/css">
        * {
            margin: 0;
            padding: 0;
        }

        ul li {
            list-style: none;
        }

        a {
            text-decoration: none;
            color: #333;
        }

        #menu {
            font: bold 16px "malgun gothic";
            width: 450px;
            height: 50px;
            background: #e0dddd;
            color: black;
            line-height: 50px;
            margin: 0 auto;
            text-align: center;
        }

        #menu>ul>li {
            float: left;
            width: 140px;
            position: relative;
        }

        #menu>ul>li>ul {
            width: 130px;
            display: none;
            position: absolute;
            font-size: 14px;
            background: rgb(229, 243, 248);
        }

        #menu>ul>li:hover>ul {
            display: block;
        }

        .slideshow-container {
            position: relative;
            max-width: 1000px;
            margin: auto;
            border: 2px solid rgb(209, 206, 206);
            border-radius: 15px;
            padding: 10px;
            box-shadow: 1px 1px 2px 6px rgb(209, 206, 206);
        }

        .video-slide {
            display: none;
        }

        .prev, .next {
            cursor: pointer;
            position: absolute;
            top: 50%;
            width: auto;
            margin-top: -22px;
            padding: 16px;
            color: white;
            font-weight: bold;
            font-size: 18px;
            transition: 0.6s ease;
            border-radius: 0 3px 3px 0;
            user-select: none;
        }

        .next {
            right: 0;
            border-radius: 3px 0 0 3px;
        }

        .prev {
            left: 0;
            border-radius: 3px 3px 3px 0;
        }

        .prev:hover, .next:hover {
            background-color: rgba(0, 0, 0, 0.8);
        }
    </style>
</head>

<body>
    <div id="menu">
        <ul>
            <li><a href="#">3D-MOM</a>
                <ul>
                    <li><a href="#Abstract">Abstract</a></li>
                    <li><a href="#method">Method</a></li>
                </ul>
            </li>
            <li><a href="#">Qualitative Result</a>
                <ul>
                    <li><a href="#Quantitative_results_1">Paper Result 1</a></li>
                    <li><a href="#Quantitative_results_2">Paper Result 2</a></li>
                </ul>
            </li>
            <li><a href="#">Visualization</a>
                <ul>
                    <li><a href="#Visualization Results">Paper Results</a></li>
                    <li><a href="#add_Visualization Results">Ablation Study</a></li>
                    <li><a href="#GUI_tutorial">GUI & Tutorial & Demo</a></li>
                </ul>
            </li>
        </ul>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
    <h1><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman'">Optimizing 4D Gaussians for Dynamic Scene Video<br>from Single Landscape Images</p></h1>
    <br/>
    <h2><p style="color:gray;font-size:0.9em;text-align:center; font-family: 'Times New Roman'">In-hwan Jin*, Haesoo Choo*, Seong-Hun Jeong, Junghwan Kim, Heemoon Park, Oh-joon Kwon, and Kyeongbo Kong</p></h2>
    <br/>
    <h2><p style="color:gray;font-size:0.5em;text-align:center; font-family: 'Times New Roman'">The symbol * signifies equal contribution.</p></h2>
    <br/>
    <h2><p style="color:gray;font-size:0.7em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">Pukyong National University, Pusan National University, DMstudio co.</p></h2>
    <br/>
    <div style="border: 2px solid white; border-radius: 15px; padding: 10px;width:1100px; margin:auto"></div>
        <!--
    <center>
        <a href="Person-in-Place_8271.pdf">
            <img src="pdf_img.png" width="40" alt="Main Paper"></a>
            &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
            <a href="https://anonymous.4open.science/r/Person-in-Place-4FF0">
                <img src="github_img.png" width="50" alt="Main Code"></a>
        
    </center>
    -->
    <br />
    <div class="slideshow-container">
        <div class="video-slide">
            <video width="1000" controls autoplay loop muted>
                <source src="static/videos/circle.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="video-slide">
            <video width="1000" controls autoplay loop muted>
                <source src="static/videos/side.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <div class="video-slide">
            <video width="1000" controls autoplay loop muted>
                <source src="static/videos/up_down.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <!-- Add more video slides as needed -->

        <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
        <a class="next" onclick="plusSlides(1)">&#10095;</a>
    </div>

    <script>
        let slideIndex = 0;
        showSlides(slideIndex);

        function plusSlides(n) {
            showSlides(slideIndex += n);
        }

        function showSlides(n) {
            let i;
            let slides = document.getElementsByClassName("video-slide");
            if (n >= slides.length) {
                slideIndex = 0;
            }
            if (n < 0) {
                slideIndex = slides.length - 1;
            }
            for (i = 0; i < slides.length; i++) {
                slides[i].style.display = "none";
            }
            slides[slideIndex].style.display = "block";
        }
    </script>

    <br/>
<!--     <center>
    <img src="static/images/NeurIPS_fig_1.png" alt="My Image" width="1000">
    <h3><p style="font-size:0.9em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">
        <strong>Figure Explain:</strong>
        Need to Explain Figure 
        <strong>(a)</strong> is blra blra
        <br/>
        <br/>
    </p></h3> -->
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
    <h2><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">Dynamic Scene Video</h1>
        <br/>
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">Recently, a field known as dynamic scene video has emerged, 
	    which creates videos with natural animations from specific camera perspectives using a combination of single image animation and 3D photography. 
	    These methods utilize Layered Depth Images (LDIs), which are created by dividing a single image into multiple layers based on depth, to represent a pseudo 3D space. 
	    However, there are limitations when attempting to discretely separate most elements, including fluids, in a continuous landscape, and 3D space cannot be fully represented this way.
	    Therefore, achieving complete 4D space virtualization through explicit representation is necessary, and we propose this approach for the first time.</p></h3>
        <br/><p id="Abstract">
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
    <h2><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">Abstract</p></h2>
    <br/>
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">To achieve realistic immersion in landscape images, 
	    fluids such as water and clouds need to move within the image while revealing new scenes from various camera perspectives. 
	    Recently, a field called dynamic scene video has emerged, which combines single image animation with 3D photography. 
	    These methods use pseudo 3D space, implicitly represented with Layered Depth Images (LDIs). 
	    LDIs separate a single image into depth-based layers, which enables elements like water and clouds to move within the image while revealing new scenes from different camera perspectives.
	    However, as landscapes typically consist of continuous elements, including fluids, the representation of a 3D space  separates a landscape image into discrete layers, 
	    and it can lead to diminished depth perception and potential distortions depending on camera movement.
	    Furthermore, due to its implicit modeling of 3D space, the output may be limited to videos in the 2D domain, potentially reducing their versatility.
	    In this paper, we propose representing a complete 3D space for dynamic scene video by modeling explicit representations, specifically 4D Gaussians, from a single image. 
	    The framework is focused on optimizing 3D Gaussians by generating multi-view images from a single image and creating 3D motion to optimize 4D Gaussians. 
	    The most important part of proposed framework is consistent 3D motion estimation, which estimates common motion among multi-view images to bring the motion in 3D space closer to actual motions. 
	    As far as we know, this is the first attempt that considers animation while representing a complete 3D space from a single landscape image.
	    Our model demonstrates the ability to provide realistic immersion in various landscape images through diverse experiments and metrics.</p></h3>
        <br/><p id="method">
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
        <br/>
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">3D-MOM Framework</p></h2>
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
        <strong>The overview of our pipeline.</strong>: Our goal is to optimize 4D Gaussians to represent a complete 3D space, including animation, from a single image.
	    <strong>(a)</strong> A depth map is estimated from the given single image, and it is converted into a point cloud. 
	    For optimizing the 3D Gaussians, multi-view RGB images are rendered according to the defined camera trajectory. 
	    <strong>(b)</strong> Similarly, multi-view motion masks are rendered for the input motion mask. 
	    These are utilized to estimate multi-view 2D motion maps along with the rendered RGB images. 
	    3D motion is obtained by unprojecting the estimated 2D motion into the 3D domain. 
	    In this context, the proposed <strong>3D Motion Optimization Module (3D-MOM)</strong> ensures consistent 3D motion across multi-views. 
	    <strong>(c)</strong> Using the optimized 3D Gaussians and generated 3D motion, 4D Gaussians are optimized for changes in position, rotation, and scaling over time.</p></h3>
        <br/>
        <img src="static/images/NeurIPS_fig_1.png" alt="My Image" width="1000">
        <br/>
	    <br>
	    <br>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
	    <strong>3D Motion Optimization Module.</strong> To maintain consistency of motion across multi-views, 3D motion is defined from the point cloud and projected into 2D images using camera parameters. 
	    The L1 loss between the projected motion and the estimated motion map as the ground truth is computed, minimizing the sum of losses for multi-view to optimize the 3D motion.
    </p></h3>
        <center><img src="static/images/NeurIPS_fig_2.png" alt="My Image" width="700"></center>
        <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
        
        </p></h3>
            <br/>


    <br/><p id="Quantitative_results_1"></p>
    </div>
    </div>


    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">

        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">Quantitative Results</p></h2>
    <br/>
    
        <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
            <strong>Quantitative comparison with existing Dynamic Scene Video models: </strong>
          	In Table I, we show the quantitative results of our method compared to other baselines on reference and non-reference metrics. 
		Our approach outperforms the other baseline on all metrics in the context of view generation. 
		In particular, our method achieved the highest scores in PSNR, SSIM, and LPIPS, indicating that the generated views are of high fidelity and perceptually similar to the ground truth views.
		Furthermore, we demonstrated that our proposed method out-performs existing methods on non-reference metrics by locally
            <br/>
        <br/>
        <center><img src="static/images/quanti_1.png" alt="My Image" width="1000"></center>


        <br/><p id="Quantitative_results_2"></p>
    </div>
    </div>


    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">

        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">Quantitative Object Interactive Skeleton Results</p></h2>
    <br/>
    Table 2 provides a comparison of results for the proposed loss scale. 
    Applying our loss scale shows improvement in object interactive skeleton evaluation performance across all models. 
    Additionally, Table 3 presents a performance comparison of the proposed Associative Attention mechanism. 
    It is evident that our method significantly outperforms modules such as conventional attention or attention + GNN (Graph Neural Network).
        <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
            <br/>
        <br/>
        <center><img src="CVPR_2024_jpg/result_qunti_v1.jpg" alt="My Image" width="500"><img src="CVPR_2024_jpg/result_qunti_v2.jpg" alt="My Image" width="500"></center>

<br/><p id="Visualization Results"></p>
    <br/>
    </div>
    </div>

    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">Visualization Results</p></h2>
    <br/>
    <img src="CVPR_2024_jpg/main_results_v1.jpg" alt="My Image" width="1000">
    
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
        <strong>(Top)</strong>: Qualitative results when generating a single person using CoModGAN, 
        Instruct-Pix2Pix, Stable-Diffusion Inpainting (SD-Inpainting). 
        <strong>(Top left)</strong> Incomplete or no humans are generated using other models. 
        <strong>(Top right)</strong> Even though humans are generated, the misaligned or non-interactive humans are synthesized. 
        <strong>(Bottom)</strong> : Demonstration of image editing with SD-inpainting, SDXL-inpainting and ours. 
        Other models did not generate a human even using a guided skeleton.        
    <br/>

    <br/><p id="add_Visualization Results"></p>
    <br/>
    </div>
    </div>

    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">Additional Visualization Results</p></h2>
    <br/>
    <img src="CVPR_2024_jpg/result_skt.jpg" alt="My Image" width="1000">
    
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
        <strong>Comparison between attention mechanism and associative attention mechanism:</strong> 
        By employing our associative attention mechanism, the overall shape of skeleton becomes more natural as shown in images of a baby sitting on the bed or a man sitting on the yacht. 
        Moreover, our associative attention mechanism generates more object-interactive poses, 
        e.g. swinging, sitting, typing, as joints of the skeleton approach the object through propagation process. 
        Moreover, even in scenarios with multiple objects, a natural skeleton is generated while interacting with the specified object.  
        
    <br/>
    <img src="CVPR_2024_jpg/test_input_supp_2.jpg" alt="My Image" width="1000">
    <img src="CVPR_2024_jpg/test_input_supp_3.jpg" alt="My Image" width="1000">
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
        <strong>Comparion of CoModGAN, Instruct-Pix2Pix, SD-Inpainting and Ours:</strong>
        We use a person bounding box, an object bounding box and a text prompt for HOI image editing. 
        In most cases of the visualization results, CoModGAN and Instruct-Pix2Pix perform poorly in generating natural humans. 
        Our results exhibit more object-interactive images than SD-Inpainting, as shown in cases of a woman with wearing a polka-dotted umbrella or a man surfing with the waves. 
        For an example, in the case of 'A woman in pajamas using her laptop on the stove top in the kitchen', 
        CoModGAN and SD-Inpainting did not generate even a human shape. Instruct-Pix2Pix failed to maintain the original image, 
        while our method generated a natural woman that matches the text prompt.
        <img src="CVPR_2024_jpg/test_input_supp_5.jpg" alt="My Image" width="1000">
        <img src="CVPR_2024_jpg/test_input_supp_6.jpg" alt="My Image" width="1000">
        <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
        <strong>Performing on multi-people images:</strong> 
        This figure shows HOI-edited images of multiple people using SD-Inpainting, SDXL-Inpainting and Ours. 
        In the first and second column, SD-Inpainting and SDXL-Inpainting fail to generate a human when a person bounding box is provided in a small size. 
        On the other hand, our method generates natural HOI images regardless of a size of a person bounding box, 
        since it utilizes object-interactive skeletons. As shown in the fourth column, 
        people sitting on the sofa and children sitting on the sofa are generated naturally with our method.
</body>
</html>
